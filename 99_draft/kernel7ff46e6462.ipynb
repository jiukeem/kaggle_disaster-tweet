{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "1) 데이터 가져오기\n",
    "\n",
    "2) EDA, preprocessing - 빈칸 등 처리하고 데이터 다듬기(cleaning)\n",
    "\n",
    "3) tokenization - 어떤 거 쓸지? 여러가지 써보자\n",
    "\n",
    "4) one-hot encoding\n",
    "\n",
    "5) word embedding - word2vec 써보기 GloVe도 써보고 싶다! 5번 생략하고 one-hot vector로 바로 돌려보고 비교해보는 것도?\n",
    "\n",
    "6) modelling - LSTM + BRNN / training - optimizer 선택\n",
    "\n",
    "7) evaluation\n",
    "\n",
    "Q. 2,3,5,6 번의 선택지를 조합하면 너무 많은데 (한개의 모델 내에서 하이퍼파라미터를 조정하는 걸 차치하고) 이 중에서 어떻게 가장 좋은 성능을 내는 걸 찾아내지? 다해보기엔 너무 많은 것 같은데.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "print('training set shape = {}'.format(df_train.shape))\n",
    "print('test set shape = {}'.format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-1. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "# 데이터프레임을 .profile_report() 한 줄로 탐색할 수 있음!\n",
    "\n",
    "pr = df_train.profile_report()\n",
    "pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keyword 컬럼의 비어있는 셀 0.8프로와 location의 33프로를 어떻게 처리할 것인가?\n",
    "\n",
    "셀 데이터 형태를 살펴보고, 살려서 가지고 갈 수 있는 방법이 있는지도 공부해야함. 일단 지금은 모델을 끝까지 짜보는게 목표라서 location은 빼버리고 keyword null 값을 가지는 example은 제외하는 방향으로 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고한 노트북은 location은 남아있는 데이터들도 너무 지저분해서 feature로 사용하지 않음. keyword는 데이터가 깨끗하고 output과 명확한 상관관계가 나타난다고 판단.(밑의 그래프에서 확인 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target_mean'] = df_train.groupby('keyword')['target'].transform('mean')\n",
    "# target_mean 컬럼을 추가할건데 target 컬럼을 keyword 필터로 정렬하는 컬럼. 뒤에 transform은 뭐더라?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(8,72))\n",
    "sns.countplot(y=df_train.sort_values(by='target_mean', ascending = False)['keyword'],\n",
    "             hue = df_train.sort_values(by='target_mean', ascending = False)['target'])\n",
    "\n",
    "plt.legend(loc=1)\n",
    "plt.show()\n",
    "\n",
    "df_train.drop(columns=['target_mean'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 텍스트에 대한 EDA 도 진행. 해당 노트북은 disaster tweet이 non-disaster tweet 보다 긴 단어(단어 한개의 평균 길이가 더 길다는 건지, 트윗의 길이가 더 길다는 건지는 잘 모르겠다)를 사용하고, 타이포가 적을 것라고 추측하여 진행함(공식 news agencies 가 올린 트윗들이 있기 때문에). 다음과 같은 meta features 사용\n",
    "\n",
    "* `word_count` number of words in text\n",
    "* `unique_word_count` number of unique words in text\n",
    "* `stop_word_count` number of stop words in text\n",
    "* `url_count` number of urls in text\n",
    "* `mean_word_length` average character count in words\n",
    "* `char_count` number of characters in text\n",
    "* `punctuation_count` number of punctuations in text\n",
    "* `hashtag_count` number of hashtags (**#**) in text\n",
    "* `mention_count` number of mentions (**@**) in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count\n",
    "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# unique_word_count, set이 무슨 기능이지?\n",
    "df_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n",
    "df_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# stop_word_count, wordcloud의 STOPWORDS 사용 (nltk.corpus의 stopwords 도 한번 써보기)\n",
    "from wordcloud import STOPWORDS\n",
    "df_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "df_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "\n",
    "# url_count\n",
    "df_train['url_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "df_test['url_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "\n",
    "# mean_word_length\n",
    "df_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# character_count\n",
    "df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n",
    "df_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# punctuation_count, string의 punctuation 사용\n",
    "import string\n",
    "df_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# hashtag_count\n",
    "df_train['hashtag_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "df_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "# mention_count\n",
    "df_train['mention_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "df_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n",
    "                'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\n",
    "disaster_tweets = df_train['target'] == 1\n",
    "\n",
    "# 이 밑으로는 제대로 이해 못했다. 파이플롯 공부 좀 더해\n",
    "fig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(20, 50), dpi=100)\n",
    "\n",
    "for i, feature in enumerate(METAFEATURES):\n",
    "    sns.distplot(df_train.loc[~disaster_tweets][feature], label='Not Disaster', ax=axes[i][0], color='green')\n",
    "    sns.distplot(df_train.loc[disaster_tweets][feature], label='Disaster', ax=axes[i][0], color='red')\n",
    "\n",
    "    sns.distplot(df_train[feature], label='Training', ax=axes[i][1])\n",
    "    sns.distplot(df_test[feature], label='Test', ax=axes[i][1])\n",
    "    \n",
    "    for j in range(2):\n",
    "        axes[i][j].set_xlabel('')\n",
    "        axes[i][j].tick_params(axis='x', labelsize=12)\n",
    "        axes[i][j].tick_params(axis='y', labelsize=12)\n",
    "        axes[i][j].legend()\n",
    "    \n",
    "    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n",
    "    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "똑같이 복사 붙여넣기 했는데 왜 에러나냐고?????ㅜㅜ\n",
    "You have categorical data, but your model needs something numerical. See our one hot encoding tutorial for a solution. \n",
    "\n",
    "어쨌든 2-1까지는 다음 노트북을 참고함\n",
    "https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert (이 노트북에서는 embedding 후에 text cleaning을 하던데 이게 어떻게 되지?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length', \n",
    "                       'char_count', 'punctuation_count', 'hashtag_count', 'mention_count'], inplace = True)\n",
    "df_test.drop(columns=['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length', \n",
    "                      'char_count', 'punctuation_count', 'hashtag_count', 'mention_count'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-2. 데이터 전처리\n",
    "\n",
    "트윗이라서 이상한 줄임말 + 대문자 남용 + @,# 등의 특수문자가 너무 많은데 이 부분 어떻게 처리할지.\n",
    "stemmization 같은 걸 해야하나? 해도 의미가 없는지 아니면 트윗이라서 오히려 더 필요한지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.concat([df_train, df_test])\n",
    "df_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url 제거\n",
    "# 정규표현식 모둘 re 사용\n",
    "import re\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"\n",
    "remove_URL(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'] = df_total['text'].apply(lambda x: remove_URL(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html 제거\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'] = df_total['text'].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이모지(emojis) 제거\n",
    "# 여기 코드는 이해 못함.. 위에도 이해는 하는데 직접 쓰라면 못쓸듯. 나중에 re 직접 작성해보기\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoji(\"Omg another Earthquake 😔😔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'] = df_total['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation 제거\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "print(remove_punct(\"I am a #king\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'] = df_total['text'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spellchecker를 이용해서 스펠링 커렉션\n",
    "\n",
    "!pip install pyspellchecker\n",
    "#어떤 패키지에 이걸 진행하는 거지? 바로 임포트 할 수 있는 패키지와 뭐가 다르죵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spelling(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split()) # text를 스페이스로 split해서 그 중 스펠체커가 인식못하는 애들 리스트\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word)) # text의 단어가 misspelled_words 에 속하는 애면 correction 진행 후 append\n",
    "        else:\n",
    "            corrected_text.append(word) # 그게 아니면 변경없이 그대로 append\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "correct_spelling('corect me plese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 셀 돌리는데 너무 오래 걸리는디\n",
    "\n",
    "df_total['text'] = df_total['text'].apply(lambda x: correct_spelling(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-2 중 여기까지는 다음 노트북을 참고함 https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'].head(20)\n",
    "\n",
    "# 지금까지 한거 중간확인. 전부 소문자로 변환하고 불용어 제거할 것. \n",
    "# 다른 노트북 보면 숫자도 다 없애던데 숫자는 남겨두는게 더 낫지 않나?\n",
    "# 스페이스가 두개인 부분도 처리해줘야 하나?\n",
    "# 저 gooooooooaaaaaaal 같은건 어떻게허냐...;_; - spellchecker가 제대로 돌아갔으면 처리됐겠지?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 앞에서 정제한 데이터들로 1. 소문자로 변경, 2. 불용어 제거를 진행한다. 근데 소문자로 변경을 스펠체크보다 먼저 해야할거 같은데.. 순서 잘못한것 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'] = df_total['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'] = df_total['text'].apply(lambda x: correct_spelling(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(text):\n",
    "    result = []\n",
    "    for word in text.split():\n",
    "        if word not in STOPWORDS:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "delete_stopwords('I am coming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text_stopwords'] = df_total['text'].apply(lambda x: delete_stopwords(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text_stopwords'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 저렇게 나와....? 일단 이거 말고 다른 notebook에서 가져온거 쓰자. 소문자화, 불용어처리, lemmatization 한번에 진행하는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def massage_text(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    ## remove anything other then characters and put everything in lowercase\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.split()\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lem = WordNetLemmatizer()\n",
    "    tweet = [lem.lemmatize(word) for word in tweet\n",
    "             if word not in set(stopwords.words('english'))]\n",
    "    tweet = ' '.join(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['clean_text'] = df_total['text'].apply(lambda x: massage_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 참고하지 않았지만 cleaning 에 대해 쉽게 잘 써놓은 노트북 https://www.kaggle.com/vanshjatana/a-simple-guide-to-text-cleaning 나중에 꼭 보기!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df_total['tokens']= df_total['clean_text'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Word2Vec (word embedding)\n",
    "\n",
    "4. one-hot encoding을 미리 진행해놓을 필요 없이 패키지가 거기서부터 한꺼번에 처리해주는 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "def fn_pre_process_data(text):\n",
    "    for word in text:\n",
    "        yield gensim.utils.simple_preprocess(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(fn_pre_process_data(df_total['clean_text']))\n",
    "\n",
    "# 이 함수 역할은 잘 모르겠지만 왜 tokens 컬럼이 아니라 clean_text를 이용해서 corpus를 만들지? \n",
    "# 내가 아는 word2vec은 token 을 one-hot vector로 만든 다음에 LM을 통해서 embedding matrix 를 구하는건뎅.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = Word2Vec(corpus, size = 150, window = 3, min_count = 2)\n",
    "wv_model.train(corpus, total_examples = len(corpus), epochs=10)\n",
    "# 요 부분이 뉴럴넷 LM로 학습시키는 부분인듯? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(token_list, vector, k=150):\n",
    "    if len(token_list) < 1: #토큰 리스트가 없을경우\n",
    "        return np.zeros(k)\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in token_list]\n",
    "        \n",
    "    sum = np.sum(vectorized, axis=0)\n",
    "    return sum/len(vectorized) #몽가... 평균값을 리턴\n",
    "\n",
    "def get_embeddings(token,vector):\n",
    "    embeddings = token.apply(lambda x: get_word_embeddings(x, wv_model))\n",
    "    return list(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_embeddings = get_embeddings(df_total['tokens'], wv_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(total_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지는 다음 노트북을 참고함. https://www.kaggle.com/slatawa/simple-implementation-of-word2vec\n",
    "\n",
    "https://www.kaggle.com/parthplc/word2vec-tutorial 에서 Word2Vec from scratch 를 다루고 있는 듯 하니 나중에 한번 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = total_embeddings[:7613]\n",
    "test_embeddings = total_embeddings[7613:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_total = train_embeddings\n",
    "y_total = df_train['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_total, y_total, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the official tokenization script created by the Google team\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(bert_layer, max_len=160)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "train_history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....? 난 왜 샘플이 8개야?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')\n",
    "test_pred = model.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = test_pred.round().astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다들 이렇게 제출하던데 왜 난 안되는겨. 한번에 되는게 하나도 없냐!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_pred.round().astype(int)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id': df_test['id'], 'target': predictions})\n",
    "output.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델링 부분에서 참고한 노트북 https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub 하나도 이해못하고 그냥 갖다 붙여넣기한거ㅜㅜ\n",
    "\n",
    "LSTM은 하지도 못했는데?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챕터마다 서로 다른 노트북을 참고하다 보니 output하고 input이 잘 안이어지는 것 같다. output은 매트릭스를 줬는데 다음 input에는 1D 리스트를 넣어줘야 한다든지, 얘는 딕셔너리를 줬는데 리스트로 변경해서 모델에 넣어야 한다든지. 그래서 뒤로 갈수록 제대로 된 결과가 안나오는 듯함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
