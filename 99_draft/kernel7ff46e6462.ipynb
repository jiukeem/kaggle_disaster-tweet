{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "1) ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "2) EDA, preprocessing - ë¹ˆì¹¸ ë“± ì²˜ë¦¬í•˜ê³  ë°ì´í„° ë‹¤ë“¬ê¸°(cleaning)\n",
    "\n",
    "3) tokenization - ì–´ë–¤ ê±° ì“¸ì§€? ì—¬ëŸ¬ê°€ì§€ ì¨ë³´ì\n",
    "\n",
    "4) one-hot encoding\n",
    "\n",
    "5) word embedding - word2vec ì¨ë³´ê¸° GloVeë„ ì¨ë³´ê³  ì‹¶ë‹¤! 5ë²ˆ ìƒëµí•˜ê³  one-hot vectorë¡œ ë°”ë¡œ ëŒë ¤ë³´ê³  ë¹„êµí•´ë³´ëŠ” ê²ƒë„?\n",
    "\n",
    "6) modelling - LSTM + BRNN / training - optimizer ì„ íƒ\n",
    "\n",
    "7) evaluation\n",
    "\n",
    "Q. 2,3,5,6 ë²ˆì˜ ì„ íƒì§€ë¥¼ ì¡°í•©í•˜ë©´ ë„ˆë¬´ ë§ì€ë° (í•œê°œì˜ ëª¨ë¸ ë‚´ì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ëŠ” ê±¸ ì°¨ì¹˜í•˜ê³ ) ì´ ì¤‘ì—ì„œ ì–´ë–»ê²Œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ê±¸ ì°¾ì•„ë‚´ì§€? ë‹¤í•´ë³´ê¸°ì—” ë„ˆë¬´ ë§ì€ ê²ƒ ê°™ì€ë°.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "print('training set shape = {}'.format(df_train.shape))\n",
    "print('test set shape = {}'.format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-1. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "# ë°ì´í„°í”„ë ˆì„ì„ .profile_report() í•œ ì¤„ë¡œ íƒìƒ‰í•  ìˆ˜ ìˆìŒ!\n",
    "\n",
    "pr = df_train.profile_report()\n",
    "pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keyword ì»¬ëŸ¼ì˜ ë¹„ì–´ìˆëŠ” ì…€ 0.8í”„ë¡œì™€ locationì˜ 33í”„ë¡œë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•  ê²ƒì¸ê°€?\n",
    "\n",
    "ì…€ ë°ì´í„° í˜•íƒœë¥¼ ì‚´í´ë³´ê³ , ì‚´ë ¤ì„œ ê°€ì§€ê³  ê°ˆ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆëŠ”ì§€ë„ ê³µë¶€í•´ì•¼í•¨. ì¼ë‹¨ ì§€ê¸ˆì€ ëª¨ë¸ì„ ëê¹Œì§€ ì§œë³´ëŠ”ê²Œ ëª©í‘œë¼ì„œ locationì€ ë¹¼ë²„ë¦¬ê³  keyword null ê°’ì„ ê°€ì§€ëŠ” exampleì€ ì œì™¸í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì°¸ê³ í•œ ë…¸íŠ¸ë¶ì€ locationì€ ë‚¨ì•„ìˆëŠ” ë°ì´í„°ë“¤ë„ ë„ˆë¬´ ì§€ì €ë¶„í•´ì„œ featureë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ. keywordëŠ” ë°ì´í„°ê°€ ê¹¨ë—í•˜ê³  outputê³¼ ëª…í™•í•œ ìƒê´€ê´€ê³„ê°€ ë‚˜íƒ€ë‚œë‹¤ê³  íŒë‹¨.(ë°‘ì˜ ê·¸ë˜í”„ì—ì„œ í™•ì¸ ê°€ëŠ¥)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target_mean'] = df_train.groupby('keyword')['target'].transform('mean')\n",
    "# target_mean ì»¬ëŸ¼ì„ ì¶”ê°€í• ê±´ë° target ì»¬ëŸ¼ì„ keyword í•„í„°ë¡œ ì •ë ¬í•˜ëŠ” ì»¬ëŸ¼. ë’¤ì— transformì€ ë­ë”ë¼?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(8,72))\n",
    "sns.countplot(y=df_train.sort_values(by='target_mean', ascending = False)['keyword'],\n",
    "             hue = df_train.sort_values(by='target_mean', ascending = False)['target'])\n",
    "\n",
    "plt.legend(loc=1)\n",
    "plt.show()\n",
    "\n",
    "df_train.drop(columns=['target_mean'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ í…ìŠ¤íŠ¸ì— ëŒ€í•œ EDA ë„ ì§„í–‰. í•´ë‹¹ ë…¸íŠ¸ë¶ì€ disaster tweetì´ non-disaster tweet ë³´ë‹¤ ê¸´ ë‹¨ì–´(ë‹¨ì–´ í•œê°œì˜ í‰ê·  ê¸¸ì´ê°€ ë” ê¸¸ë‹¤ëŠ” ê±´ì§€, íŠ¸ìœ—ì˜ ê¸¸ì´ê°€ ë” ê¸¸ë‹¤ëŠ” ê±´ì§€ëŠ” ì˜ ëª¨ë¥´ê² ë‹¤)ë¥¼ ì‚¬ìš©í•˜ê³ , íƒ€ì´í¬ê°€ ì ì„ ê²ƒë¼ê³  ì¶”ì¸¡í•˜ì—¬ ì§„í–‰í•¨(ê³µì‹ news agencies ê°€ ì˜¬ë¦° íŠ¸ìœ—ë“¤ì´ ìˆê¸° ë•Œë¬¸ì—). ë‹¤ìŒê³¼ ê°™ì€ meta features ì‚¬ìš©\n",
    "\n",
    "* `word_count` number of words in text\n",
    "* `unique_word_count` number of unique words in text\n",
    "* `stop_word_count` number of stop words in text\n",
    "* `url_count` number of urls in text\n",
    "* `mean_word_length` average character count in words\n",
    "* `char_count` number of characters in text\n",
    "* `punctuation_count` number of punctuations in text\n",
    "* `hashtag_count` number of hashtags (**#**) in text\n",
    "* `mention_count` number of mentions (**@**) in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count\n",
    "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# unique_word_count, setì´ ë¬´ìŠ¨ ê¸°ëŠ¥ì´ì§€?\n",
    "df_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n",
    "df_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# stop_word_count, wordcloudì˜ STOPWORDS ì‚¬ìš© (nltk.corpusì˜ stopwords ë„ í•œë²ˆ ì¨ë³´ê¸°)\n",
    "from wordcloud import STOPWORDS\n",
    "df_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "df_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "\n",
    "# url_count\n",
    "df_train['url_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "df_test['url_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "\n",
    "# mean_word_length\n",
    "df_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# character_count\n",
    "df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n",
    "df_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# punctuation_count, stringì˜ punctuation ì‚¬ìš©\n",
    "import string\n",
    "df_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# hashtag_count\n",
    "df_train['hashtag_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "df_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "# mention_count\n",
    "df_train['mention_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "df_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n",
    "                'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\n",
    "disaster_tweets = df_train['target'] == 1\n",
    "\n",
    "# ì´ ë°‘ìœ¼ë¡œëŠ” ì œëŒ€ë¡œ ì´í•´ ëª»í–ˆë‹¤. íŒŒì´í”Œë¡¯ ê³µë¶€ ì¢€ ë”í•´\n",
    "fig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(20, 50), dpi=100)\n",
    "\n",
    "for i, feature in enumerate(METAFEATURES):\n",
    "    sns.distplot(df_train.loc[~disaster_tweets][feature], label='Not Disaster', ax=axes[i][0], color='green')\n",
    "    sns.distplot(df_train.loc[disaster_tweets][feature], label='Disaster', ax=axes[i][0], color='red')\n",
    "\n",
    "    sns.distplot(df_train[feature], label='Training', ax=axes[i][1])\n",
    "    sns.distplot(df_test[feature], label='Test', ax=axes[i][1])\n",
    "    \n",
    "    for j in range(2):\n",
    "        axes[i][j].set_xlabel('')\n",
    "        axes[i][j].tick_params(axis='x', labelsize=12)\n",
    "        axes[i][j].tick_params(axis='y', labelsize=12)\n",
    "        axes[i][j].legend()\n",
    "    \n",
    "    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n",
    "    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë˜‘ê°™ì´ ë³µì‚¬ ë¶™ì—¬ë„£ê¸° í–ˆëŠ”ë° ì™œ ì—ëŸ¬ë‚˜ëƒê³ ?????ã…œã…œ\n",
    "You have categorical data, but your model needs something numerical. See our one hot encoding tutorial for a solution. \n",
    "\n",
    "ì–´ì¨Œë“  2-1ê¹Œì§€ëŠ” ë‹¤ìŒ ë…¸íŠ¸ë¶ì„ ì°¸ê³ í•¨\n",
    "https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert (ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” embedding í›„ì— text cleaningì„ í•˜ë˜ë° ì´ê²Œ ì–´ë–»ê²Œ ë˜ì§€?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length', \n",
    "                       'char_count', 'punctuation_count', 'hashtag_count', 'mention_count'], inplace = True)\n",
    "df_test.drop(columns=['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length', \n",
    "                      'char_count', 'punctuation_count', 'hashtag_count', 'mention_count'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-2. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "íŠ¸ìœ—ì´ë¼ì„œ ì´ìƒí•œ ì¤„ì„ë§ + ëŒ€ë¬¸ì ë‚¨ìš© + @,# ë“±ì˜ íŠ¹ìˆ˜ë¬¸ìê°€ ë„ˆë¬´ ë§ì€ë° ì´ ë¶€ë¶„ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€.\n",
    "stemmization ê°™ì€ ê±¸ í•´ì•¼í•˜ë‚˜? í•´ë„ ì˜ë¯¸ê°€ ì—†ëŠ”ì§€ ì•„ë‹ˆë©´ íŠ¸ìœ—ì´ë¼ì„œ ì˜¤íˆë ¤ ë” í•„ìš”í•œì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.concat([df_train, df_test])\n",
    "df_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url ì œê±°\n",
    "# ì •ê·œí‘œí˜„ì‹ ëª¨ë‘˜ re ì‚¬ìš©\n",
    "import re\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"\n",
    "remove_URL(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'] = df_total['text'].apply(lambda x: remove_URL(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html ì œê±°\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'] = df_total['text'].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ëª¨ì§€(emojis) ì œê±°\n",
    "# ì—¬ê¸° ì½”ë“œëŠ” ì´í•´ ëª»í•¨.. ìœ„ì—ë„ ì´í•´ëŠ” í•˜ëŠ”ë° ì§ì ‘ ì“°ë¼ë©´ ëª»ì“¸ë“¯. ë‚˜ì¤‘ì— re ì§ì ‘ ì‘ì„±í•´ë³´ê¸°\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoji(\"Omg another Earthquake ğŸ˜”ğŸ˜”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'] = df_total['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation ì œê±°\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "print(remove_punct(\"I am a #king\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'] = df_total['text'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spellcheckerë¥¼ ì´ìš©í•´ì„œ ìŠ¤í ë§ ì»¤ë ‰ì…˜\n",
    "\n",
    "!pip install pyspellchecker\n",
    "#ì–´ë–¤ íŒ¨í‚¤ì§€ì— ì´ê±¸ ì§„í–‰í•˜ëŠ” ê±°ì§€? ë°”ë¡œ ì„í¬íŠ¸ í•  ìˆ˜ ìˆëŠ” íŒ¨í‚¤ì§€ì™€ ë­ê°€ ë‹¤ë¥´ì£µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spelling(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split()) # textë¥¼ ìŠ¤í˜ì´ìŠ¤ë¡œ splití•´ì„œ ê·¸ ì¤‘ ìŠ¤í ì²´ì»¤ê°€ ì¸ì‹ëª»í•˜ëŠ” ì• ë“¤ ë¦¬ìŠ¤íŠ¸\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word)) # textì˜ ë‹¨ì–´ê°€ misspelled_words ì— ì†í•˜ëŠ” ì• ë©´ correction ì§„í–‰ í›„ append\n",
    "        else:\n",
    "            corrected_text.append(word) # ê·¸ê²Œ ì•„ë‹ˆë©´ ë³€ê²½ì—†ì´ ê·¸ëŒ€ë¡œ append\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "correct_spelling('corect me plese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ ì…€ ëŒë¦¬ëŠ”ë° ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ëŠ”ë””\n",
    "\n",
    "df_total['text'] = df_total['text'].apply(lambda x: correct_spelling(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-2 ì¤‘ ì—¬ê¸°ê¹Œì§€ëŠ” ë‹¤ìŒ ë…¸íŠ¸ë¶ì„ ì°¸ê³ í•¨ https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'].head(20)\n",
    "\n",
    "# ì§€ê¸ˆê¹Œì§€ í•œê±° ì¤‘ê°„í™•ì¸. ì „ë¶€ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  ë¶ˆìš©ì–´ ì œê±°í•  ê²ƒ. \n",
    "# ë‹¤ë¥¸ ë…¸íŠ¸ë¶ ë³´ë©´ ìˆ«ìë„ ë‹¤ ì—†ì• ë˜ë° ìˆ«ìëŠ” ë‚¨ê²¨ë‘ëŠ”ê²Œ ë” ë‚«ì§€ ì•Šë‚˜?\n",
    "# ìŠ¤í˜ì´ìŠ¤ê°€ ë‘ê°œì¸ ë¶€ë¶„ë„ ì²˜ë¦¬í•´ì¤˜ì•¼ í•˜ë‚˜?\n",
    "# ì € gooooooooaaaaaaal ê°™ì€ê±´ ì–´ë–»ê²Œí—ˆëƒ...;_; - spellcheckerê°€ ì œëŒ€ë¡œ ëŒì•„ê°”ìœ¼ë©´ ì²˜ë¦¬ëê² ì§€?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ì•ì—ì„œ ì •ì œí•œ ë°ì´í„°ë“¤ë¡œ 1. ì†Œë¬¸ìë¡œ ë³€ê²½, 2. ë¶ˆìš©ì–´ ì œê±°ë¥¼ ì§„í–‰í•œë‹¤. ê·¼ë° ì†Œë¬¸ìë¡œ ë³€ê²½ì„ ìŠ¤í ì²´í¬ë³´ë‹¤ ë¨¼ì € í•´ì•¼í• ê±° ê°™ì€ë°.. ìˆœì„œ ì˜ëª»í•œê²ƒ ê°™ë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'] = df_total['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text'] = df_total['text'].apply(lambda x: correct_spelling(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(text):\n",
    "    result = []\n",
    "    for word in text.split():\n",
    "        if word not in STOPWORDS:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "delete_stopwords('I am coming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text_stopwords'] = df_total['text'].apply(lambda x: delete_stopwords(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['text_stopwords'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì™œ ì €ë ‡ê²Œ ë‚˜ì™€....? ì¼ë‹¨ ì´ê±° ë§ê³  ë‹¤ë¥¸ notebookì—ì„œ ê°€ì ¸ì˜¨ê±° ì“°ì. ì†Œë¬¸ìí™”, ë¶ˆìš©ì–´ì²˜ë¦¬, lemmatization í•œë²ˆì— ì§„í–‰í•˜ëŠ”ê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def massage_text(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    ## remove anything other then characters and put everything in lowercase\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.split()\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lem = WordNetLemmatizer()\n",
    "    tweet = [lem.lemmatize(word) for word in tweet\n",
    "             if word not in set(stopwords.words('english'))]\n",
    "    tweet = ' '.join(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['clean_text'] = df_total['text'].apply(lambda x: massage_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì—¬ê¸°ì„œëŠ” ì°¸ê³ í•˜ì§€ ì•Šì•˜ì§€ë§Œ cleaning ì— ëŒ€í•´ ì‰½ê²Œ ì˜ ì¨ë†“ì€ ë…¸íŠ¸ë¶ https://www.kaggle.com/vanshjatana/a-simple-guide-to-text-cleaning ë‚˜ì¤‘ì— ê¼­ ë³´ê¸°!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df_total['tokens']= df_total['clean_text'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Word2Vec (word embedding)\n",
    "\n",
    "4. one-hot encodingì„ ë¯¸ë¦¬ ì§„í–‰í•´ë†“ì„ í•„ìš” ì—†ì´ íŒ¨í‚¤ì§€ê°€ ê±°ê¸°ì„œë¶€í„° í•œêº¼ë²ˆì— ì²˜ë¦¬í•´ì£¼ëŠ” ê²ƒ ê°™ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "def fn_pre_process_data(text):\n",
    "    for word in text:\n",
    "        yield gensim.utils.simple_preprocess(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(fn_pre_process_data(df_total['clean_text']))\n",
    "\n",
    "# ì´ í•¨ìˆ˜ ì—­í• ì€ ì˜ ëª¨ë¥´ê² ì§€ë§Œ ì™œ tokens ì»¬ëŸ¼ì´ ì•„ë‹ˆë¼ clean_textë¥¼ ì´ìš©í•´ì„œ corpusë¥¼ ë§Œë“¤ì§€? \n",
    "# ë‚´ê°€ ì•„ëŠ” word2vecì€ token ì„ one-hot vectorë¡œ ë§Œë“  ë‹¤ìŒì— LMì„ í†µí•´ì„œ embedding matrix ë¥¼ êµ¬í•˜ëŠ”ê±´ë….."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = Word2Vec(corpus, size = 150, window = 3, min_count = 2)\n",
    "wv_model.train(corpus, total_examples = len(corpus), epochs=10)\n",
    "# ìš” ë¶€ë¶„ì´ ë‰´ëŸ´ë„· LMë¡œ í•™ìŠµì‹œí‚¤ëŠ” ë¶€ë¶„ì¸ë“¯? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(token_list, vector, k=150):\n",
    "    if len(token_list) < 1: #í† í° ë¦¬ìŠ¤íŠ¸ê°€ ì—†ì„ê²½ìš°\n",
    "        return np.zeros(k)\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in token_list]\n",
    "        \n",
    "    sum = np.sum(vectorized, axis=0)\n",
    "    return sum/len(vectorized) #ëª½ê°€... í‰ê· ê°’ì„ ë¦¬í„´\n",
    "\n",
    "def get_embeddings(token,vector):\n",
    "    embeddings = token.apply(lambda x: get_word_embeddings(x, wv_model))\n",
    "    return list(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_embeddings = get_embeddings(df_total['tokens'], wv_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(total_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì—¬ê¸°ê¹Œì§€ëŠ” ë‹¤ìŒ ë…¸íŠ¸ë¶ì„ ì°¸ê³ í•¨. https://www.kaggle.com/slatawa/simple-implementation-of-word2vec\n",
    "\n",
    "https://www.kaggle.com/parthplc/word2vec-tutorial ì—ì„œ Word2Vec from scratch ë¥¼ ë‹¤ë£¨ê³  ìˆëŠ” ë“¯ í•˜ë‹ˆ ë‚˜ì¤‘ì— í•œë²ˆ ë³´ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = total_embeddings[:7613]\n",
    "test_embeddings = total_embeddings[7613:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_total = train_embeddings\n",
    "y_total = df_train['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_total, y_total, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the official tokenization script created by the Google team\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(bert_layer, max_len=160)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "train_history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....? ë‚œ ì™œ ìƒ˜í”Œì´ 8ê°œì•¼?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')\n",
    "test_pred = model.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = test_pred.round().astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ë“¤ ì´ë ‡ê²Œ ì œì¶œí•˜ë˜ë° ì™œ ë‚œ ì•ˆë˜ëŠ”ê²¨. í•œë²ˆì— ë˜ëŠ”ê²Œ í•˜ë‚˜ë„ ì—†ëƒ!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_pred.round().astype(int)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id': df_test['id'], 'target': predictions})\n",
    "output.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ë§ ë¶€ë¶„ì—ì„œ ì°¸ê³ í•œ ë…¸íŠ¸ë¶ https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub í•˜ë‚˜ë„ ì´í•´ëª»í•˜ê³  ê·¸ëƒ¥ ê°–ë‹¤ ë¶™ì—¬ë„£ê¸°í•œê±°ã…œã…œ\n",
    "\n",
    "LSTMì€ í•˜ì§€ë„ ëª»í–ˆëŠ”ë°?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì±•í„°ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ ë…¸íŠ¸ë¶ì„ ì°¸ê³ í•˜ë‹¤ ë³´ë‹ˆ outputí•˜ê³  inputì´ ì˜ ì•ˆì´ì–´ì§€ëŠ” ê²ƒ ê°™ë‹¤. outputì€ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ì¤¬ëŠ”ë° ë‹¤ìŒ inputì—ëŠ” 1D ë¦¬ìŠ¤íŠ¸ë¥¼ ë„£ì–´ì¤˜ì•¼ í•œë‹¤ë“ ì§€, ì–˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ì¤¬ëŠ”ë° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€ê²½í•´ì„œ ëª¨ë¸ì— ë„£ì–´ì•¼ í•œë‹¤ë“ ì§€. ê·¸ë˜ì„œ ë’¤ë¡œ ê°ˆìˆ˜ë¡ ì œëŒ€ë¡œ ëœ ê²°ê³¼ê°€ ì•ˆë‚˜ì˜¤ëŠ” ë“¯í•¨.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
